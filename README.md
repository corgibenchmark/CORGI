
# CORGI: A New Text-to-SQL Benchmark for the Business Domain

> **Agent Bain vs. Agent McKinsey: A New Text-to-SQL Benchmark for the Business Domain**  

---

## üåê Overview

**CORGI** is a new **text-to-SQL benchmark** specifically designed for **real-world business intelligence (BI)** contexts.  
Unlike existing datasets (e.g., Spider, WikiSQL, BIRD), CORGI moves beyond factual retrieval and challenges models to perform **causal reasoning**, **forecasting**, and **strategic recommendation** ‚Äî mirroring real management consulting workflows.

> The benchmark draws inspiration from enterprises such as **Doordash**, **Airbnb**, and **Lululemon**, covering 10 major verticals across consumer platforms, retail, and digital services.

---

## üìä Key Contributions

- üè¢ **Business-Inspired Databases** ‚Äî 10 synthetic but realistic databases modeled after real industries (food delivery, rentals, fashion, analytics, etc.)  
- üí¨ **Four Categories of Business Queries**  

  | Type | Description | Example |
  |------|--------------|----------|
  | 1 | **Descriptive** | ‚ÄúWhat were the total sales on September 1, 2025?‚Äù |
  | 2 | **Explanatory** | ‚ÄúWhy did the revenue of Pop Mart NYC go down last month?‚Äù |
  | 3 | **Predictive** | ‚ÄúHow many Labubu will be sold online next quarter?‚Äù |
  | 4 | **Recommendational** | ‚ÄúHow can we expand the Labubu market in Europe next year?‚Äù |

- ‚öôÔ∏è **Atomized Multi-Agent Evaluation Framework** ‚Äî a discriminator agent orchestrates seven specialized scoring agents evaluating structure, data sense, insightfulness, and compliance.  
- üß† **Business Literature-Inspired Evaluation** ‚Äî derived from **MBA consulting frameworks** (McKinsey, BCG, Bain).

---

## üß© Framework Overview

![Atomized Evaluation Framework](./assets/framework.png)

*Figure 1: Multi-agent evaluation framework integrating business logic with schema-driven data simulation.*

---

## üí° Query Taxonomy

![Query Categories](figures/4Q.pdf)

*Figure 2: Four categories of business questions from descriptive retrieval to strategic recommendation.*

---

## üß™ Methodology

1. **Schema Construction** ‚Äî Extract operational logic from public business sources and construct relational schemas.  
2. **Data Simulation Rules** ‚Äî Define **Business Operation Rules**, **Latent Feature Distributions**, and **Latent Seasonal Trends**.  
3. **Question Generation** ‚Äî Generate and refine questions using Gemini 2.5‚ÄìFlash-Lite.  
4. **Evaluation** ‚Äî Combine **agent-based evaluation** with **human pairwise comparison** (Bradley‚ÄìTerry framework).

---

## üìà Benchmark Statistics

| Domain | Example Company | Model | Tables | Foreign Keys |
|---------|-----------------|--------|---------|--------------|
| Food Delivery | Doordash | B2C/B2B2C | 30 | 25 |
| Home Rental | Airbnb | C2C | 18 | 25 |
| Clothing | Lululemon | B2C | 34 | 40 |
| Freelancing | Upwork | C2C/C2B | 25 | 33 |
| App Store | Google Play | B2C | 18 | 22 |
| Car Rental | Turo | C2C | 30 | 68 |
| Luxury Consignment | The RealReal | C2C | 30 | 65 |
| Advertising Analytics | Ux_ad | B2B | 20 | 25 |
| Personalized Product | Persona Nutrition | B2C | 28 | 38 |
| E-Commerce Enablement | Shopify | B2B | 23 | 60 |

> CORGI databases contain **~26 tables per schema**, significantly larger and more complex than prior benchmarks like **BIRD (7.3)**.

---

## ‚öñÔ∏è Evaluation Dimensions

| Dimension | Description |
|------------|--------------|
| **Structure** | Logical organization and argument coherence |
| **SQL SER** | Execution success rate of generated SQL |
| **Data Sense** | Correctness of data interpretation |
| **Insightfulness** | Ability to uncover causal or strategic insights |
| **Operational Implementability** | Practicality of proposed business actions |
| **Purpose Alignment** | Alignment with company‚Äôs goals |
| **Compliance** | Ethical and regulatory soundness (for Type 4) |

---

## üßÆ Results Summary

- **CORGI is 21% harder than BIRD**, reflecting higher reasoning difficulty.  
- **LLMs struggle with Type 4 (Recommendational)** tasks, indicating limits in strategic planning.  
- GPT-4o and Gemini 2.5 achieve SQL SER between **57‚Äì75%**, significantly below simple retrieval benchmarks.

---

## üåç Access

- **Dataset & Evaluation Framework:** [github.com/corgibenchmark/CORGI](https://github.com/corgibenchmark/CORGI)  
- **Online Evaluation Platform:** [txt2sql.com](https://txt2sql.com)  
- **More Info:** [bird-bench.github.io](https://bird-bench.github.io/)

---

## üßæ Citation

If you use CORGI in your research, please cite:

```bibtex
@article{corgi2025,
  title={Agent Bain vs. Agent McKinsey: A New Text-to-SQL Benchmark for the Business Domain},
  author={Anonymous},
  year={2025},
  journal={ACL Submission},
}
```

---

## ‚ö†Ô∏è Limitations & Ethics

- Databases are **synthetic** ‚Äî privacy-safe but lack natural noise.  
- Only **single-turn** questions are supported in v1.  
- Each query has a single canonical NL formulation (no paraphrase robustness).  
- Includes built-in **ethical evaluation dimension** for responsible recommendations.

---
